{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membaca dataset\n",
    "data_nilai = pd.read_csv('dataset/Sample Nilai Alumni Prodi Informatika_Ver1.csv')\n",
    "data_profesi = pd.read_csv('dataset/Sample Profesi Pekerjaan Alumni Prodi Informatika Universitas Gunadarma_Ver1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nilai.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_profesi.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the columns 'KDMK' and 'JENIS' from the 'nilai_alumni' dataset\n",
    "nilai_alumni_cleaned = data_nilai.drop(columns=['KDMK', 'JENIS'])\n",
    "\n",
    "# Remove the column 'Nama Lengkap' from the 'data_profesi' dataset\n",
    "data_profesi_cleaned = data_profesi.drop(columns=['Nama Lengkap'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two datasets based on 'NPM' and perform one-hot encoding on the 'NAMA MK' (subject names) column\n",
    "merged_data = pd.merge(data_profesi_cleaned, nilai_alumni_cleaned, on='NPM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see the count of each grade\n",
    "grade_counts = merged_data['NILAI'].value_counts()\n",
    "grade_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the data type of the 'NAMA MK' column to INDEX\n",
    "merged_data['INDEX'] = merged_data['NAMA MK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pivots the merged data table to create a table with the student's NPM, IPK, Klasifikasi Profesi as rows, and the subject indices as columns, with the student's grades as the values.\n",
    "\n",
    "# The `pivot_table` function is used to reshape the data from a long format to a wide format, with the subject indices as the column names and the grades as the values. The `first` aggregation function is used to handle any duplicate grades for a student in a subject.\n",
    "\n",
    "# The resulting `pivoted_grades` DataFrame will have the student information (NPM, IPK, Klasifikasi Profesi) as the index, and the subject grades as the columns, with 0 filled in for any subjects not taken by a student.\n",
    "\n",
    "\n",
    "pivoted_grades = merged_data.pivot_table(\n",
    "    index=['NPM', 'IPK', 'Klasifikasi Profesi'], \n",
    "    columns='INDEX', \n",
    "    values='NILAI', \n",
    "    aggfunc='first'  # Take the first occurrence if there are duplicates\n",
    ").reset_index()\n",
    "\n",
    "# Merge the pivoted table back to the original data to align with the rest of the information\n",
    "# This will leave 0 for subjects not taken by each student\n",
    "\n",
    "pivoted_grades.fillna(0, inplace=True)\n",
    "\n",
    "pivoted_grades.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values with 0 for the grade columns\n",
    "pivoted_grades_filled = pivoted_grades.fillna(0)\n",
    "\n",
    "# Display the updated data with NaN replaced by 0\n",
    "pivoted_grades_filled.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grade mapping\n",
    "grade_mapping = {'A': 4, 'B': 3, 'C': 2, 'D': 1,}\n",
    "\n",
    "# Apply the grade mapping to all relevant columns that contain grades\n",
    "# We'll apply this mapping to all columns except 'NPM', 'IPK', 'Klasifikasi', and 'Profesi'\n",
    "\n",
    "# Selecting only grade columns\n",
    "grade_columns = pivoted_grades_filled.columns.difference(['NPM', 'IPK', 'Klasifikasi', 'Profesi'])\n",
    "\n",
    "# Apply grade mapping to these columns\n",
    "pivoted_grades_filled[grade_columns] = pivoted_grades_filled[grade_columns].replace(grade_mapping)\n",
    "\n",
    "\n",
    "# Display the updated dataframe\n",
    "pivoted_grades_filled.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random NPM from the data\n",
    "random_npm = pivoted_grades_filled['NPM'].sample().values[0]\n",
    "\n",
    "print(f\"Randomly selected NPM: {random_npm}\")\n",
    "\n",
    "# Get the data for this random NPM\n",
    "random_student_data = pivoted_grades_filled[pivoted_grades_filled['NPM'] == random_npm]\n",
    "\n",
    "# Display the data for the randomly selected student\n",
    "random_student_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a binary mask where 0 is True and non-zero is False\n",
    "mask = pivoted_grades_filled.isnull()\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Create the heatmap\n",
    "sns.heatmap(mask, cmap='binary', cbar=False, yticklabels=False)\n",
    "\n",
    "# Set the title\n",
    "plt.title('Missing Values Heatmap')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eksplorasi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Basic statistics of the numerical columns\n",
    "basic_stats = pivoted_grades_filled.describe()\n",
    "\n",
    "# Step 2: Distribution of predicted professions\n",
    "profession_distribution = pivoted_grades_filled['Klasifikasi Profesi'].value_counts()\n",
    "\n",
    "basic_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profession_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = pivoted_grades_filled.isnull().sum()\n",
    "\n",
    "# Check data distribution for the target column 'Profesi'\n",
    "profesi_distribution = pivoted_grades_filled['Klasifikasi Profesi'].value_counts()\n",
    "\n",
    "missing_values, profesi_distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Encode the target variable 'Klasifikasi Profesi'\n",
    "label_encoder = LabelEncoder()\n",
    "pivoted_grades_filled['Klasifikasi Profesi Encoded'] = label_encoder.fit_transform(pivoted_grades_filled['Klasifikasi Profesi'])\n",
    "\n",
    "# Exclude non-numeric columns and scale only the numeric features\n",
    "numeric_cols = pivoted_grades_filled.columns[3:-1]  # Skipping NPM, Profesi, and the new encoded column\n",
    "\n",
    "# Scale the numeric features\n",
    "scaler = StandardScaler()\n",
    "pivoted_grades_filled[numeric_cols] = scaler.fit_transform(pivoted_grades_filled[numeric_cols])\n",
    "\n",
    "# Separate the features (X) and target (y)\n",
    "X = pivoted_grades_filled[numeric_cols]\n",
    "y = pivoted_grades_filled['Klasifikasi Profesi Encoded']\n",
    "\n",
    "# Display the first few rows after scaling and encoding\n",
    "pivoted_grades_filled.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set the dimension of the latent space\n",
    "latent_dim = 100\n",
    "input_dim = X.shape[1]  # Number of features in the dataset\n",
    "\n",
    "# Build the generator\n",
    "def build_generator():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(128, input_dim=latent_dim, activation='relu'))\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dense(input_dim, activation='tanh'))\n",
    "    return model\n",
    "\n",
    "# Build the discriminator\n",
    "def build_discriminator():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(256, input_dim=input_dim, activation='relu'))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    return model\n",
    "\n",
    "# Create the generator and discriminator\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# Define the Wasserstein loss function\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return tf.keras.backend.mean(y_true * y_pred)\n",
    "\n",
    "# Compile the discriminator\n",
    "discriminator.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.00005), loss=wasserstein_loss)\n",
    "\n",
    "# Build and compile the combined GAN model\n",
    "discriminator.trainable = False\n",
    "gan_input = layers.Input(shape=(latent_dim,))\n",
    "generated_data = generator(gan_input)\n",
    "gan_output = discriminator(generated_data)\n",
    "gan_model = tf.keras.Model(gan_input, gan_output)\n",
    "gan_model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.00005), loss=wasserstein_loss)\n",
    "\n",
    "gan_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('/GPU:0')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "# Define dimensions\n",
    "latent_dim = 100\n",
    "input_dim = X.shape[1]  # Number of features in the dataset\n",
    "\n",
    "# Build generator\n",
    "def build_generator():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(128, input_dim=latent_dim, activation='relu'))\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dense(input_dim, activation='tanh'))\n",
    "    return model\n",
    "\n",
    "# Build discriminator\n",
    "def build_discriminator():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(256, input_dim=input_dim, activation='relu'))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    return model\n",
    "\n",
    "# Create models\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# Define Wasserstein loss\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return tf.keras.backend.mean(y_true * y_pred)\n",
    "\n",
    "# Compile discriminator\n",
    "discriminator.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.00005), loss=wasserstein_loss)\n",
    "\n",
    "# Create GAN model\n",
    "discriminator.trainable = False\n",
    "gan_input = layers.Input(shape=(latent_dim,))\n",
    "generated_data = generator(gan_input)\n",
    "gan_output = discriminator(generated_data)\n",
    "gan_model = tf.keras.Model(gan_input, gan_output)\n",
    "gan_model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.00005), loss=wasserstein_loss)\n",
    "\n",
    "# GAN training loop with concise output and no progress bars\n",
    "def train_gan(epochs, batch_size):\n",
    "    with tf.device('/device:GPU:0'):  # Ensures the operations are on the GPU\n",
    "        for epoch in range(epochs):\n",
    "            d_losses = []\n",
    "            a_losses = []\n",
    "            \n",
    "            # Update the critic (discriminator)\n",
    "            for _ in range(5):  # Critic updates\n",
    "                random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
    "                generated_samples = generator.predict(random_latent_vectors, verbose=0)  # Disable progress bar\n",
    "                real_samples = X.sample(batch_size).values\n",
    "                combined_samples = np.concatenate([generated_samples, real_samples])\n",
    "                labels = np.concatenate([np.ones((batch_size, 1)), -np.ones((batch_size, 1))])\n",
    "                d_loss = discriminator.train_on_batch(combined_samples, labels)\n",
    "                d_losses.append(d_loss)\n",
    "\n",
    "            # Update the generator (adversarial network)\n",
    "            random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
    "            misleading_targets = -np.ones((batch_size, 1))\n",
    "            a_loss = gan_model.train_on_batch(random_latent_vectors, misleading_targets)\n",
    "            a_losses.append(a_loss)\n",
    "\n",
    "            # Print concise output for the current epoch\n",
    "            if epoch % 100 == 0 or epoch == epochs - 1:\n",
    "                print(f\"Epoch {epoch}/{epochs} - Discriminator Loss: {np.mean(d_losses):.4f} - Adversarial Loss: {np.mean(a_losses):.4f}\")\n",
    "\n",
    "# Train the GAN\n",
    "train_gan(epochs=10000, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a batch of new data samples\n",
    "latent_vectors = np.random.normal(size=(32, latent_dim))\n",
    "generated_samples = generator.predict(latent_vectors)\n",
    "\n",
    "# Compare with real data\n",
    "real_samples = X.sample(32).values\n",
    "\n",
    "# For example, check the mean and standard deviation of real vs generated samples\n",
    "real_mean = np.mean(real_samples, axis=0)\n",
    "generated_mean = np.mean(generated_samples, axis=0)\n",
    "\n",
    "real_std = np.std(real_samples, axis=0)\n",
    "generated_std = np.std(generated_samples, axis=0)\n",
    "\n",
    "print(\"Mean difference between real and generated data:\", np.mean(np.abs(real_mean - generated_mean)))\n",
    "print(\"Std difference between real and generated data:\", np.mean(np.abs(real_std - generated_std)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate by Classifying Generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a simple classifier on real data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Now evaluate classifier on real test data and generated data\n",
    "y_pred_real = clf.predict(X_test)\n",
    "print(\"Performance on real data:\")\n",
    "print(classification_report(y_test, y_pred_real))\n",
    "\n",
    "# Generate fake data and evaluate\n",
    "generated_samples = generator.predict(np.random.normal(size=(X_test.shape[0], latent_dim)))\n",
    "y_pred_generated = clf.predict(generated_samples)\n",
    "print(\"Performance on generated data:\")\n",
    "print(classification_report(y_test, y_pred_generated))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have stored generator and discriminator losses during training\n",
    "def plot_loss(generator_loss, discriminator_loss):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(generator_loss, label='Generator Loss')\n",
    "    plt.plot(discriminator_loss, label='Discriminator Loss')\n",
    "    plt.title('Generator and Discriminator Loss During Training')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# 1. Plot loss curves\n",
    "# Example lists for generator and discriminator loss, these would come from training\n",
    "generator_loss = np.random.randn(1000)  # replace with actual generator loss values\n",
    "discriminator_loss = np.random.randn(1000)  # replace with actual discriminator loss values\n",
    "plot_loss(generator_loss, discriminator_loss)\n",
    "\n",
    "# 2. Real vs Generated Data Distribution\n",
    "def plot_distribution(real_data, generated_data, feature_name):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.kdeplot(real_data, label='Real', shade=True)\n",
    "    sns.kdeplot(generated_data, label='Generated', shade=True)\n",
    "    plt.title(f'Real vs Generated Data Distribution for {feature_name}')\n",
    "    plt.xlabel('Feature Value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example real and generated data, use your real dataset and generated samples\n",
    "real_samples = X.sample(1000).values  # real data\n",
    "latent_vectors = np.random.normal(size=(1000, latent_dim))\n",
    "generated_samples = generator.predict(latent_vectors)  # generated data\n",
    "\n",
    "# Plot for a selected feature (e.g., the first feature)\n",
    "plot_distribution(real_samples[:, 0], generated_samples[:, 0], 'Feature 1')\n",
    "\n",
    "# 3. t-SNE Visualization\n",
    "def plot_tsne(real_data, generated_data):\n",
    "    tsne = TSNE(n_components=2, perplexity=30, n_iter=300)\n",
    "    real_tsne = tsne.fit_transform(real_data)\n",
    "    generated_tsne = tsne.fit_transform(generated_data)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.scatter(real_tsne[:, 0], real_tsne[:, 1], label='Real Data', alpha=0.6)\n",
    "    plt.scatter(generated_tsne[:, 0], generated_tsne[:, 1], label='Generated Data', alpha=0.6)\n",
    "    plt.title('t-SNE Visualization of Real and Generated Data')\n",
    "    plt.xlabel('Dimension 1')\n",
    "    plt.ylabel('Dimension 2')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Apply t-SNE on the first 1000 real and generated samples\n",
    "plot_tsne(real_samples[:1000], generated_samples[:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Model with WGAN-GP Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "# Set up your generator and discriminator models\n",
    "def build_generator():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(256, input_dim=latent_dim, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(1024, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(input_dim, activation='tanh'))\n",
    "    return model\n",
    "\n",
    "def build_discriminator():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(1024, input_dim=input_dim))\n",
    "    model.add(layers.LeakyReLU(0.2))\n",
    "    model.add(layers.Dense(512))\n",
    "    model.add(layers.LeakyReLU(0.2))\n",
    "    model.add(layers.Dense(256))\n",
    "    model.add(layers.LeakyReLU(0.2))\n",
    "    model.add(layers.Dense(1))\n",
    "    return model\n",
    "\n",
    "# Wasserstein Loss\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return tf.keras.backend.mean(y_true * y_pred)\n",
    "\n",
    "# Gradient Penalty\n",
    "def gradient_penalty(real_samples, generated_samples, batch_size, critic):\n",
    "    alpha = tf.random.uniform([batch_size, 1], 0.0, 1.0)\n",
    "    interpolated = alpha * real_samples + (1 - alpha) * generated_samples\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(interpolated)\n",
    "        prediction = critic(interpolated)\n",
    "    \n",
    "    gradients = tape.gradient(prediction, interpolated)\n",
    "    gradient_l2_norm = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=[1]))\n",
    "    gradient_penalty = tf.reduce_mean((gradient_l2_norm - 1.0) ** 2)\n",
    "    \n",
    "    return gradient_penalty\n",
    "\n",
    "# Model Training Loop (with GPU acceleration)\n",
    "def train_wgan_gp(generator, discriminator, epochs, batch_size, latent_dim, X_train, lambda_gp=10):\n",
    "    # Create optimizers\n",
    "    generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.5)\n",
    "    discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.5)\n",
    "    \n",
    "    # Begin training loop\n",
    "    for epoch in range(epochs):\n",
    "        for _ in range(5):  # Critic (Discriminator) updates more frequently than Generator\n",
    "            real_samples = X_train.sample(batch_size).values\n",
    "            latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
    "            generated_samples = generator.predict(latent_vectors)\n",
    "            \n",
    "            with tf.device('/GPU:0'):  # Ensure operations run on GPU\n",
    "                # Train the critic\n",
    "                with tf.GradientTape() as tape:\n",
    "                    real_pred = discriminator(real_samples)\n",
    "                    generated_pred = discriminator(generated_samples)\n",
    "                    \n",
    "                    gp = gradient_penalty(real_samples, generated_samples, batch_size, discriminator)\n",
    "                    critic_loss = tf.reduce_mean(generated_pred) - tf.reduce_mean(real_pred) + lambda_gp * gp\n",
    "                \n",
    "                grads = tape.gradient(critic_loss, discriminator.trainable_weights)\n",
    "                discriminator_optimizer.apply_gradients(zip(grads, discriminator.trainable_weights))\n",
    "\n",
    "        # Train the generator\n",
    "        latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
    "        with tf.device('/GPU:0'):\n",
    "            with tf.GradientTape() as tape:\n",
    "                generated_samples = generator(latent_vectors)\n",
    "                generated_pred = discriminator(generated_samples)\n",
    "                generator_loss = -tf.reduce_mean(generated_pred)\n",
    "            \n",
    "            grads = tape.gradient(generator_loss, generator.trainable_weights)\n",
    "            generator_optimizer.apply_gradients(zip(grads, generator.trainable_weights))\n",
    "\n",
    "        # Logging the progress\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}/{epochs}, Critic Loss: {critic_loss.numpy()}, Generator Loss: {generator_loss.numpy()}\")\n",
    "\n",
    "# Set parameters\n",
    "latent_dim = 100  # Dimension of the latent space\n",
    "input_dim = X_train.shape[1]  # Number of input features\n",
    "epochs = 10000\n",
    "batch_size = 64\n",
    "\n",
    "# Build generator and discriminator\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# Start training WGAN with GPU acceleration\n",
    "train_wgan_gp(generator, discriminator, epochs, batch_size, latent_dim, X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval WGAN-GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss(generator_loss, critic_loss):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(generator_loss, label='Generator Loss')\n",
    "    plt.plot(critic_loss, label='Critic Loss')\n",
    "    plt.title('Generator and Critic Loss During Training')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Example lists of losses over epochs\n",
    "generator_loss = [0.5, 0.3, 0.2, 0.15, 0.12]  # replace with actual loss values\n",
    "critic_loss = [-0.7, -0.5, -0.35, -0.3, -0.25]  # replace with actual loss values\n",
    "\n",
    "plot_loss(generator_loss, critic_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Example function to plot real vs generated distributions\n",
    "def plot_feature_distribution(real_data, generated_data, feature_name):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.kdeplot(real_data, label='Real Data', shade=True)\n",
    "    sns.kdeplot(generated_data, label='Generated Data', shade=True)\n",
    "    plt.title(f'Distribution of {feature_name}: Real vs Generated')\n",
    "    plt.xlabel('Feature Value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Replace with actual feature columns from real and generated samples\n",
    "real_samples = X_train.sample(1000).values[:, 0]  # First feature of real samples\n",
    "latent_vectors = np.random.normal(size=(1000, latent_dim))\n",
    "generated_samples = generator.predict(latent_vectors)\n",
    "generated_samples_feature = generated_samples[:, 0]  # First feature of generated samples\n",
    "\n",
    "plot_feature_distribution(real_samples, generated_samples_feature, \"Feature 1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_tsne(real_data, generated_data, title='t-SNE Visualization'):\n",
    "    tsne = TSNE(n_components=2, perplexity=30, n_iter=300)\n",
    "    real_tsne = tsne.fit_transform(real_data)\n",
    "    generated_tsne = tsne.fit_transform(generated_data)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(real_tsne[:, 0], real_tsne[:, 1], label='Real Data', alpha=0.5)\n",
    "    plt.scatter(generated_tsne[:, 0], generated_tsne[:, 1], label='Generated Data', alpha=0.5)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Run t-SNE on real and generated samples\n",
    "real_samples_tsne = X_train.sample(1000).values  # Sample real data\n",
    "generated_samples_tsne = generator.predict(np.random.normal(size=(1000, latent_dim)))\n",
    "\n",
    "plot_tsne(real_samples_tsne, generated_samples_tsne, title=\"t-SNE: Real vs Generated Data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_generated_samples(generator, latent_dim, n_samples=1000, title=\"Generated Samples\"):\n",
    "    latent_vectors = np.random.normal(size=(n_samples, latent_dim))\n",
    "    generated_samples = generator.predict(latent_vectors)\n",
    "    \n",
    "    # For visualization, we could use the first 2 dimensions if possible\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(generated_samples[:, 0], generated_samples[:, 1], alpha=0.6, color='blue')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Call this function after training for several epochs\n",
    "plot_generated_samples(generator, latent_dim, title=\"Generated Samples After Tuning\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_generated_samples_over_epochs(generator, latent_dim, n_epochs, n_samples=1000):\n",
    "    for epoch in range(0, n_epochs, 100):  # Plot every 100 epochs\n",
    "        latent_vectors = np.random.normal(size=(n_samples, latent_dim))\n",
    "        generated_samples = generator.predict(latent_vectors)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(generated_samples[:, 0], generated_samples[:, 1], alpha=0.6, label=f'Epoch {epoch}')\n",
    "        plt.title(f'Generated Samples at Epoch {epoch}')\n",
    "        plt.xlabel('Feature 1')\n",
    "        plt.ylabel('Feature 2')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# Track the diversity of generated samples over epochs\n",
    "plot_generated_samples_over_epochs(generator, latent_dim, n_epochs=1000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eksplorasi-wgan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
